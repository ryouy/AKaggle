{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 85723,
          "databundleVersionId": 10652996,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30822,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "PS5E1 | Sticker Sales Ensemble XGBoost LightGBM",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryouy/Kaggle/blob/main/test2/connectPS5E1_%7C_Sticker_Sales_Ensemble_XGBoost_LightGBM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "61b9KfW7Qzr9"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "playground_series_s5e1_path = kagglehub.competition_download('playground-series-s5e1')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "lRx33PvTQzr-"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"1\"></a>\n",
        "# <div style=\"text-align:center; border-radius:15px 50px; padding:7px; color:white; margin:0; font-size:110%; font-family:Pacifico; background-color:#0073e6; overflow:hidden\"><b> Machine learning - Sticker Sales Machine learning Ensemble</b></div>\n",
        "\n",
        "![](https://img.freepik.com/fotos-gratis/conceito-corporativo-de-discussao-de-reuniao-de-executivos_53876-121054.jpg?t=st=1736174095~exp=1736177695~hmac=1674968bdaf762b81a1b638fc9605c09af343cc2bd58a1ebc566da832bc5cf13&w=740)"
      ],
      "metadata": {
        "id": "HZxEMOacQzr-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"1\"></a>\n",
        "# <div style=\"text-align:center; border-radius:15px 50px; padding:7px; color:white; margin:0; font-size:110%; font-family:Pacifico; background-color:#0073e6; overflow:hidden\"><b> Part 1 - Business Problem </b></div>\n",
        "\n",
        "**Business Problem**  \n",
        "\n",
        "The goal is to develop a machine learning model that accurately forecasts the number of Kaggle-branded sticker sales across different stores and countries. The predictions should consider temporal patterns, seasonality, holidays, and other potential trends inherent in the synthetic dataset. Achieving accurate predictions will require a balance of feature engineering and model selection, with submissions evaluated using the Mean Absolute Percentage Error (MAPE).\n",
        "\n",
        "This problem aims to simulate real-world challenges in sales forecasting, providing an opportunity to test and refine forecasting skills in a controlled environment."
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-01-03T19:06:46.559209Z",
          "iopub.execute_input": "2025-01-03T19:06:46.559587Z",
          "iopub.status.idle": "2025-01-03T19:06:46.563687Z",
          "shell.execute_reply.started": "2025-01-03T19:06:46.559559Z",
          "shell.execute_reply": "2025-01-03T19:06:46.562188Z"
        },
        "id": "yWoP-sT7Qzr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing packages\n",
        "!pip install watermark"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:56:22.182855Z",
          "iopub.execute_input": "2025-01-07T04:56:22.183312Z"
        },
        "id": "lcMmqo96Qzr_",
        "outputId": "77b56cb2-177b-4e06-c121-918de39b5055"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7e6fe40db340>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/watermark/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7e6fe40db6d0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/watermark/\u001b[0m\u001b[33m\n\u001b[0m",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Import of libraries\n",
        "\n",
        "# System libraries\n",
        "import re\n",
        "import unicodedata\n",
        "import itertools\n",
        "\n",
        "# Library for file manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas\n",
        "\n",
        "# Data visualization\n",
        "import seaborn as sns\n",
        "import matplotlib.pylab as pl\n",
        "import matplotlib as m\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Configuration for graph width and layout\n",
        "sns.set_theme(style='whitegrid')\n",
        "palette='viridis'\n",
        "\n",
        "# Warnings remove alerts\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Python version\n",
        "from platform import python_version\n",
        "print('Python version in this Jupyter Notebook:', python_version())\n",
        "\n",
        "# Load library versions\n",
        "import watermark\n",
        "\n",
        "# Library versions\n",
        "%reload_ext watermark\n",
        "%watermark -a \"Library versions\" --iversions"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:47.631208Z",
          "iopub.execute_input": "2025-01-07T04:47:47.631501Z",
          "iopub.status.idle": "2025-01-07T04:47:49.448572Z",
          "shell.execute_reply.started": "2025-01-07T04:47:47.631474Z",
          "shell.execute_reply": "2025-01-07T04:47:49.444482Z"
        },
        "id": "q6scXL4WQzsA",
        "outputId": "0ee0d0d4-e521-4851-9914-bd3c9ddf7251"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Python version in this Jupyter Notebook: 3.10.12\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6782238972ac>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Load library versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwatermark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Library versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'watermark'"
          ],
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'watermark'",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"1\"></a>\n",
        "# <div style=\"text-align:center; border-radius:15px 50px; padding:7px; color:white; margin:0; font-size:110%; font-family:Pacifico; background-color:#0073e6; overflow:hidden\"><b> Part 2 - Database</b></div>"
      ],
      "metadata": {
        "id": "GSq8NeyTQzsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Database\n",
        "train_df = pd.read_csv('/kaggle/input/playground-series-s5e1/train.csv', parse_dates=['date'])\n",
        "test_df = pd.read_csv('/kaggle/input/playground-series-s5e1/test.csv', parse_dates=['date'])\n",
        "\n",
        "# Viewing dataset\n",
        "train_df"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.449174Z",
          "iopub.status.idle": "2025-01-07T04:47:49.449564Z",
          "shell.execute_reply": "2025-01-07T04:47:49.449424Z"
        },
        "id": "Dm19vwrZQzsB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Viewing first 5 data\n",
        "train_df.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.450665Z",
          "iopub.status.idle": "2025-01-07T04:47:49.451145Z",
          "shell.execute_reply": "2025-01-07T04:47:49.450944Z"
        },
        "id": "q3g2YLKAQzsB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Viewing 5 latest data\n",
        "train_df.tail()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.45191Z",
          "iopub.status.idle": "2025-01-07T04:47:49.452359Z",
          "shell.execute_reply": "2025-01-07T04:47:49.452166Z"
        },
        "id": "nwGcjs3VQzsB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Info data\n",
        "train_df.info()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.45312Z",
          "iopub.status.idle": "2025-01-07T04:47:49.453421Z",
          "shell.execute_reply": "2025-01-07T04:47:49.453295Z"
        },
        "id": "4cISqy2HQzsC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Type data\n",
        "train_df.dtypes"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.454036Z",
          "iopub.status.idle": "2025-01-07T04:47:49.454326Z",
          "shell.execute_reply": "2025-01-07T04:47:49.454212Z"
        },
        "id": "BVUFEKzVQzsC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Viewing rows and columns\n",
        "train_df.shape"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.45543Z",
          "iopub.status.idle": "2025-01-07T04:47:49.455919Z",
          "shell.execute_reply": "2025-01-07T04:47:49.455669Z"
        },
        "id": "ooENZLBqQzsC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"1\"></a>\n",
        "# <div style=\"text-align:center; border-radius:15px 50px; padding:7px; color:white; margin:0; font-size:110%; font-family:Pacifico; background-color:#0073e6; overflow:hidden\"><b> Part 3 - Data cleaning</b></div>"
      ],
      "metadata": {
        "id": "pnpbl5g9QzsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Checking for missing values in each column:\")\n",
        "print(train_df.isnull().sum())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.456931Z",
          "iopub.status.idle": "2025-01-07T04:47:49.457305Z",
          "shell.execute_reply": "2025-01-07T04:47:49.457172Z"
        },
        "id": "rTJWtz7JQzsC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the number of null values ​​in specific columns\n",
        "print(train_df[['num_sold']].isnull().sum())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.458204Z",
          "iopub.status.idle": "2025-01-07T04:47:49.458643Z",
          "shell.execute_reply": "2025-01-07T04:47:49.458432Z"
        },
        "id": "n_8SiQtqQzsD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop 'id' column\n",
        "train_df = train_df.drop(['id'], axis=1)\n",
        "train_df"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.459579Z",
          "iopub.status.idle": "2025-01-07T04:47:49.459883Z",
          "shell.execute_reply": "2025-01-07T04:47:49.459765Z"
        },
        "id": "7vyv_NArQzsD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_outliers(df, columns, min_rows=10):\n",
        "    \"\"\"\n",
        "    Removes outliers from the specified numeric columns using the IQR method,\n",
        "    ensuring that at least `min_rows` remain in the DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    df (DataFrame): The DataFrame from which outliers will be removed.\n",
        "    columns (list): A list of column names where outliers should be removed.\n",
        "    min_rows (int): The minimum number of rows that should remain in the DataFrame\n",
        "                    after removing outliers. Default is 10.\n",
        "\n",
        "    Returns:\n",
        "    DataFrame: A DataFrame with outliers removed from the specified columns.\n",
        "    \"\"\"\n",
        "\n",
        "    for col in columns:\n",
        "        # Attempt to convert the column to numeric, ignoring errors\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "        # Fill NaN values with the median of the column (or another appropriate method)\n",
        "        df[col].fillna(df[col].median(), inplace=True)\n",
        "\n",
        "        # Calculate the first and third quartiles and the interquartile range (IQR)\n",
        "        Q1 = df[col].quantile(0.25)\n",
        "        Q3 = df[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "        # Apply the outlier filter only if it leaves at least `min_rows` in the DataFrame\n",
        "        filtered_df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
        "        if filtered_df.shape[0] >= min_rows:\n",
        "            df = filtered_df\n",
        "        else:\n",
        "            # Print a message if the filter would remove too many rows\n",
        "            print(f\"Column: {col} - Unable to apply outlier filter without removing all rows.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Applying the function to the train_df DataFrame\n",
        "numeric_columns = ['num_sold']\n",
        "train_df = remove_outliers(train_df, numeric_columns)\n",
        "\n",
        "train_df.head(n=20)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.460704Z",
          "iopub.status.idle": "2025-01-07T04:47:49.460985Z",
          "shell.execute_reply": "2025-01-07T04:47:49.460871Z"
        },
        "id": "t9gr8b9KQzsD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have a function for data cleaning and outlier removal\n",
        "def clean_and_remove_outliers(df):\n",
        "    # Apply data cleaning and outlier removal\n",
        "    # Example:\n",
        "    # df = df.dropna()  # Remove missing values\n",
        "    # df = df[df['column_name'] < threshold]  # Remove outliers based on a threshold\n",
        "    return df\n",
        "\n",
        "# Apply the same transformations to the test set\n",
        "X_test_kaggle = clean_and_remove_outliers(test_df.drop(columns=['id']))  # Removing only the 'id' column\n",
        "\n",
        "def remove_outliers(df, columns):\n",
        "    for col in columns:\n",
        "        Q1 = df[col].quantile(0.25)\n",
        "        Q3 = df[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
        "    return df\n",
        "\n",
        "# Applying outlier removal to the correct columns in the test set\n",
        "# Replace with the correct column names\n",
        "X_test_kaggle = remove_outliers(test_df.drop(columns=['id']), columns=['date'])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.461874Z",
          "iopub.status.idle": "2025-01-07T04:47:49.462198Z",
          "shell.execute_reply": "2025-01-07T04:47:49.462078Z"
        },
        "id": "FhjOyoFRQzsD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the number of null values ​​in specific columns\n",
        "print(train_df[['num_sold']].isnull().sum())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.463075Z",
          "iopub.status.idle": "2025-01-07T04:47:49.463401Z",
          "shell.execute_reply": "2025-01-07T04:47:49.463266Z"
        },
        "id": "cbMVi679QzsE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Checking for missing values in each column:\")\n",
        "print(train_df.isnull().sum())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.464049Z",
          "iopub.status.idle": "2025-01-07T04:47:49.464333Z",
          "shell.execute_reply": "2025-01-07T04:47:49.464217Z"
        },
        "id": "P9BBI910QzsE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy data\n",
        "data = train_df.copy()\n",
        "\n",
        "# Salvando dataset\n",
        "data.to_csv(\"dataset_clear.csv\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.465186Z",
          "iopub.status.idle": "2025-01-07T04:47:49.465509Z",
          "shell.execute_reply": "2025-01-07T04:47:49.465349Z"
        },
        "id": "0TUxgFSyQzsE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"1\"></a>\n",
        "# <div style=\"text-align:center; border-radius:15px 50px; padding:7px; color:white; margin:0; font-size:110%; font-family:Pacifico; background-color:#0073e6; overflow:hidden\"><b> Part 4 - Exploratory data analysis</b></div>"
      ],
      "metadata": {
        "id": "EAnu_7wLQzsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'date' column to datetime format\n",
        "train_df['date'] = pd.to_datetime(train_df['date'])\n",
        "test_df['date'] = pd.to_datetime(test_df['date'])\n",
        "\n",
        "# Extract temporal features\n",
        "train_df['day_of_week'] = train_df['date'].dt.dayofweek  # 0 = Monday, 6 = Sunday\n",
        "train_df['month'] = train_df['date'].dt.month\n",
        "train_df['year'] = train_df['date'].dt.year\n",
        "train_df['is_weekend'] = train_df['day_of_week'].isin([5, 6])  # Saturday, Sunday"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.466302Z",
          "iopub.status.idle": "2025-01-07T04:47:49.466607Z",
          "shell.execute_reply": "2025-01-07T04:47:49.466472Z"
        },
        "id": "RE9lVo4wQzsE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Viewing descriptive statistics\n",
        "train_df.describe()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.467535Z",
          "iopub.status.idle": "2025-01-07T04:47:49.467957Z",
          "shell.execute_reply": "2025-01-07T04:47:49.467753Z"
        },
        "id": "pZlAWMpGQzsF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the seaborn style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Histogram for the 'price' variable\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.histplot(train_df['num_sold'], kde=True, color='skyblue', bins=40, alpha=0.7, line_kws={'linewidth': 2, 'color': 'blue'})\n",
        "plt.title('Distribution of Car Prices', fontsize=16)\n",
        "plt.xlabel('Price', fontsize=14)\n",
        "plt.ylabel('Frequency', fontsize=14)\n",
        "\n",
        "# Adding mean and median lines\n",
        "mean_price = train_df['num_sold'].mean()\n",
        "median_price = train_df['num_sold'].median()\n",
        "plt.axvline(mean_price, color='red', linestyle='--', linewidth=2, label=f'Mean: ${mean_price:.2f}')\n",
        "plt.axvline(median_price, color='green', linestyle='-', linewidth=2, label=f'Median: ${median_price:.2f}')\n",
        "\n",
        "# Adding a legend\n",
        "plt.legend()\n",
        "plt.grid(False)\n",
        "\n",
        "# Displaying the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.468702Z",
          "iopub.status.idle": "2025-01-07T04:47:49.469031Z",
          "shell.execute_reply": "2025-01-07T04:47:49.468908Z"
        },
        "id": "jPdcb3GmQzsF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import holidays\n",
        "\n",
        "# Add holiday flag (example: US holidays)\n",
        "us_holidays = holidays.US()\n",
        "train_df['is_holiday'] = train_df['date'].isin(us_holidays)\n",
        "\n",
        "# Group by date to analyze trends\n",
        "daily_sales = train_df.groupby('date')['num_sold'].sum().reset_index()\n",
        "\n",
        "# Visualization 1: Time series plot of sales\n",
        "daily_sales.set_index('date')['num_sold'].plot(figsize=(12, 6), title='Daily Sales Over Time')\n",
        "plt.ylabel('Number Sold')\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.47003Z",
          "iopub.status.idle": "2025-01-07T04:47:49.470341Z",
          "shell.execute_reply": "2025-01-07T04:47:49.470198Z"
        },
        "id": "s15-Gj6QQzsF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization 2: Boxplot of sales by day of the week\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=train_df, x='day_of_week', y='num_sold',\n",
        "            palette='viridis', showfliers=False)\n",
        "plt.title('Sales Distribution by Day of the Week')\n",
        "plt.xlabel('Day of Week (0=Monday, 6=Sunday)')\n",
        "plt.ylabel('Number Sold')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.471245Z",
          "iopub.status.idle": "2025-01-07T04:47:49.471545Z",
          "shell.execute_reply": "2025-01-07T04:47:49.471413Z"
        },
        "id": "UhNVGb_EQzsF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization 3: Heatmap of monthly sales trends\n",
        "monthly_sales = train_df.groupby(['year', 'month'])['num_sold'].sum().unstack()\n",
        "plt.figure(figsize=(20, 10))\n",
        "sns.heatmap(monthly_sales, cmap='coolwarm', annot=True, fmt='.0f', linewidths=.5)\n",
        "plt.title('Monthly Sales Heatmap')\n",
        "plt.ylabel('Year')\n",
        "plt.xlabel('Month')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.472561Z",
          "iopub.status.idle": "2025-01-07T04:47:49.472943Z",
          "shell.execute_reply": "2025-01-07T04:47:49.472776Z"
        },
        "id": "uNis7aHhQzsF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering: Seasonal and cyclic features\n",
        "train_df['day_of_year'] = train_df['date'].dt.dayofyear\n",
        "train_df['sin_day_of_year'] = np.sin(2 * np.pi * train_df['day_of_year'] / 365)\n",
        "train_df['cos_day_of_year'] = np.cos(2 * np.pi * train_df['day_of_year'] / 365)\n",
        "\n",
        "# Distribution of sales\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(train_df['num_sold'], kde=True, bins=30, color='blue')\n",
        "plt.title('Distribution of Sales')\n",
        "plt.xlabel('Number Sold')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.474217Z",
          "iopub.status.idle": "2025-01-07T04:47:49.47481Z",
          "shell.execute_reply": "2025-01-07T04:47:49.474521Z"
        },
        "id": "ZsLXp6RuQzsG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap\n",
        "plt.figure(figsize=(10, 6))\n",
        "corr_matrix = train_df[['num_sold', 'day_of_week', 'month', 'year', 'is_weekend']].corr()\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.476026Z",
          "iopub.status.idle": "2025-01-07T04:47:49.476336Z",
          "shell.execute_reply": "2025-01-07T04:47:49.476205Z"
        },
        "id": "QYO9EqZOQzsG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4.1 - Outlier removal\n",
        "\n",
        "- In this chart, we can observe the presence of outliers in the target column. To ensure the quality and accuracy of the analysis, these outliers will be removed. Removing outliers is a crucial step in data preprocessing as they can distort results and negatively impact predictive models.\n",
        "\n",
        "- This removal will be performed using appropriate statistical methods, such as the interquartile range (IQR) or z-score analysis, ensuring that only the most representative data is considered in the subsequent analysis."
      ],
      "metadata": {
        "id": "D0j8O_cOQzsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the figure and axes for subplots with tight layout\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 6))  # 1 row, 3 columns\n",
        "\n",
        "# First boxplot - Fuel Type vs Price\n",
        "sns.boxplot(x=\"product\", y=\"num_sold\", data=train_df, palette=\"Set2\", ax=axes[0])\n",
        "axes[0].set_title('Price Distribution by Fuel Type', fontsize=14)\n",
        "axes[0].set_xlabel('Fuel Type', fontsize=12)\n",
        "axes[0].set_ylabel('Price (USD)', fontsize=12)\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Third boxplot - Model Year vs Price\n",
        "sns.boxplot(x=\"store\", y=\"num_sold\", data=train_df, palette=\"Set2\", ax=axes[1])\n",
        "axes[1].set_title('Price Distribution by Model Year', fontsize=14)\n",
        "axes[1].set_xlabel('Model Year', fontsize=12)\n",
        "axes[1].set_ylabel('Price (USD)', fontsize=12)\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Automatically adjust subplot parameters for better fit\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the plots\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.477299Z",
          "iopub.status.idle": "2025-01-07T04:47:49.47789Z",
          "shell.execute_reply": "2025-01-07T04:47:49.477711Z"
        },
        "id": "V1PSA3IDQzsG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "### Outlier removal\n",
        "\n",
        "# Assuming 'train_df' is the DataFrame where you want to remove the outliers\n",
        "Q1 = train_df['num_sold'].quantile(0.25)  # Calculate the first quartile (25th percentile) of the 'price' column\n",
        "Q3 = train_df['num_sold'].quantile(0.75)  # Calculate the third quartile (75th percentile) of the 'price' column\n",
        "IQR = Q3 - Q1  # Calculate the interquartile range (IQR) for 'price'\n",
        "\n",
        "# Define the boundaries to consider a value as an outlier\n",
        "lower_bound = Q1 - 1.5 * IQR  # Calculate the lower bound (below which values will be considered outliers)\n",
        "upper_bound = Q3 + 1.5 * IQR  # Calculate the upper bound (above which values will be considered outliers)\n",
        "\n",
        "# Filter out the outliers from the 'price' column\n",
        "# Keep only the data points within the bounds\n",
        "train_df = train_df[(train_df['num_sold'] >= lower_bound) & (train_df['num_sold'] <= upper_bound)]\n",
        "train_df.head(n=20)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.478554Z",
          "iopub.status.idle": "2025-01-07T04:47:49.478961Z",
          "shell.execute_reply": "2025-01-07T04:47:49.478775Z"
        },
        "id": "VJuQ-zEHQzsH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
        "\n",
        "# Boxplot\n",
        "sns.boxplot(x=train_df['num_sold'], ax=axes[0], palette=\"Set2\")\n",
        "axes[0].set_title('Boxplot of num sold - After Outlier Removal', fontsize=16)\n",
        "axes[0].set_xlabel('Price', fontsize=14)\n",
        "axes[0].grid(True, axis='y', linestyle='--', alpha=0.7)  # Adding grid lines for better readability\n",
        "\n",
        "# Histogram\n",
        "sns.histplot(train_df['num_sold'], bins=40, kde=True, ax=axes[1], palette=\"Set2\")  # Consistent color\n",
        "axes[1].set_title('Histogram of num sold - After Outlier Removal', fontsize=16)\n",
        "axes[1].set_xlabel('Price', fontsize=14)\n",
        "axes[1].set_ylabel('Frequency', fontsize=14)\n",
        "axes[1].grid(True, axis='y', linestyle='--', alpha=0.7)  # Adding grid lines\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.480171Z",
          "iopub.status.idle": "2025-01-07T04:47:49.480546Z",
          "shell.execute_reply": "2025-01-07T04:47:49.480382Z"
        },
        "id": "GczWKi1RQzsH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Here is a boxplot after the removal of outliers in the target variable. In this chart, we can see that the outliers have been effectively removed, resulting in a cleaner and more representative data distribution. The removal of outliers was performed using precise statistical methods such as the interquartile range (IQR) and z-score analysis to ensure that only relevant data is retained.\n",
        "\n",
        "- This step is crucial to ensure the accuracy of subsequent analyses and the robustness of predictive models. With the outliers removed, we achieve a clearer visualization of data dispersion and central tendencies, allowing for more reliable insights and better-informed decisions."
      ],
      "metadata": {
        "id": "hYsIt3hhQzsH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"1\"></a>\n",
        "# <div style=\"text-align:center; border-radius:15px 50px; padding:7px; color:white; margin:0; font-size:110%; font-family:Pacifico; background-color:#0073e6; overflow:hidden\"><b> Part 5 - Feature engineering</b></div>"
      ],
      "metadata": {
        "id": "kF-6p9pJQzsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create features\n",
        "def create_date_features(df):\n",
        "    train_df[\"year\"] = train_df[\"date\"].dt.year  # Extract year from the date column\n",
        "    train_df[\"month\"] = train_df[\"date\"].dt.month  # Extract month from the date column\n",
        "    train_df[\"day\"] = train_df[\"date\"].dt.day  # Extract day from the date column\n",
        "    train_df[\"dayofweek\"] = train_df[\"date\"].dt.dayofweek  # Monday=0, Sunday=6\n",
        "    train_df[\"weekofyear\"] = train_df[\"date\"].dt.isocalendar().week.astype(int)  # Week of the year\n",
        "\n",
        "    # Weekend indicator (code added here) # adicionar\n",
        "    train_df[\"is_weekend\"] = train_df[\"dayofweek\"].apply(lambda x: 1 if x >= 5 else 0)\n",
        "    return train_df\n",
        "\n",
        "# Apply date feature creation to train and test datasets # adicionar\n",
        "train_df = create_date_features(train_df)  # adicionar\n",
        "test_df = create_date_features(test_df)  # adicionar"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.481328Z",
          "iopub.status.idle": "2025-01-07T04:47:49.481801Z",
          "shell.execute_reply": "2025-01-07T04:47:49.481649Z"
        },
        "id": "8RM5-c5aQzsI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing library\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Creating the Label encoder\n",
        "Label_pre = LabelEncoder()\n",
        "data_cols=train_df.select_dtypes(exclude=['int','float']).columns\n",
        "label_col =list(data_cols)\n",
        "\n",
        "# Applying encoder\n",
        "train_df[label_col]=train_df[label_col].apply(lambda col:Label_pre.fit_transform(col))\n",
        "\n",
        "# Viewing\n",
        "Label_pre"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.482953Z",
          "iopub.status.idle": "2025-01-07T04:47:49.483327Z",
          "shell.execute_reply": "2025-01-07T04:47:49.483166Z"
        },
        "id": "e_WKQU4UQzsI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Viewing dataset after applying label encoder\n",
        "train_df.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.484099Z",
          "iopub.status.idle": "2025-01-07T04:47:49.484426Z",
          "shell.execute_reply": "2025-01-07T04:47:49.484284Z"
        },
        "id": "f_ptD5sVQzsI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"1\"></a>\n",
        "# <div style=\"text-align:center; border-radius:15px 50px; padding:7px; color:white; margin:0; font-size:110%; font-family:Pacifico; background-color:#0073e6; overflow:hidden\"><b> Part 6 - Training and testing division</b></div>"
      ],
      "metadata": {
        "id": "GEIAmqpBQzsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting variables for model\n",
        "X = train_df.drop('num_sold', axis=1)\n",
        "y = train_df['num_sold']"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.485039Z",
          "iopub.status.idle": "2025-01-07T04:47:49.485386Z",
          "shell.execute_reply": "2025-01-07T04:47:49.485228Z"
        },
        "id": "d6QQEt1iQzsJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing data x\n",
        "X.shape"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.486231Z",
          "iopub.status.idle": "2025-01-07T04:47:49.486937Z",
          "shell.execute_reply": "2025-01-07T04:47:49.486764Z"
        },
        "id": "-1EFNz5XQzsJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing data y\n",
        "y.shape"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.488182Z",
          "iopub.status.idle": "2025-01-07T04:47:49.488529Z",
          "shell.execute_reply": "2025-01-07T04:47:49.488395Z"
        },
        "id": "9q8eR15MQzsJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"1\"></a>\n",
        "# <div style=\"text-align:center; border-radius:15px 50px; padding:7px; color:white; margin:0; font-size:110%; font-family:Pacifico; background-color:#0073e6; overflow:hidden\"><b> Part 7 - Model training</b></div>"
      ],
      "metadata": {
        "id": "-gODSilzQzsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Viewing X_train rows and columns\n",
        "print(\"Viewing X train data:\", X_train.shape)\n",
        "print(\"Viewing y train data:\", y_train.shape)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.489436Z",
          "iopub.status.idle": "2025-01-07T04:47:49.489833Z",
          "shell.execute_reply": "2025-01-07T04:47:49.489648Z"
        },
        "id": "Dhrchcn_QzsJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this stage, we proceeded with model training. A crucial step in this process is splitting the data into training and testing sets. This division allows us to evaluate the model's performance on unseen data and avoid potential overfitting issues. Typically, a portion of the data is reserved for training, while another portion is held out for evaluation. Additionally, techniques such as cross-validation can be applied to ensure a more robust assessment of the model.\n",
        "\n",
        "During training, the models are exposed to the training data, adjusting their parameters to optimize performance with respect to the chosen metric, such as accuracy or mean squared error. After training, the models are evaluated using the testing data to assess their generalization ability. This step is crucial to ensure that the model can make accurate predictions on new data. The model training process involves choosing and properly configuring algorithms, selecting hyperparameters, and continuously evaluating the model's performance on different datasets."
      ],
      "metadata": {
        "id": "O0vzAMStQzsJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"1\"></a>\n",
        "# <div style=\"text-align:center; border-radius:15px 50px; padding:7px; color:white; margin:0; font-size:110%; font-family:Pacifico; background-color:#0073e6; overflow:hidden\"><b> Part 8 - Model training</b></div>"
      ],
      "metadata": {
        "id": "c7wIi04cQzsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from lightgbm import log_evaluation\n",
        "from math import sqrt\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# XGBoost model parameters with GPU and other adjustments\n",
        "xgb_params = {'tree_method': 'gpu_hist',           # Use GPU-optimized tree method\n",
        "              'predictor': 'gpu_predictor',        # Use GPU for prediction\n",
        "              'objective': 'reg:squarederror',     # Objective function for regression\n",
        "              'n_estimators': 1000,                # Number of trees (estimators)\n",
        "              'learning_rate': 0.05,               # Learning rate\n",
        "              'max_depth': 6,                      # Maximum depth of trees\n",
        "              'subsample': 0.8,                    # Ratio of samples for subsampling\n",
        "              'colsample_bytree': 0.8,             # Proportion of columns for subsampling\n",
        "              'reg_alpha': 0.1,                    # L1 Regularization\n",
        "              'reg_lambda': 0.1,                   # L2 Regularization\n",
        "              'verbosity': 1                       # Verbosity level for output (1 for detailed output)\n",
        "             }\n",
        "\n",
        "# LightGBM model parameters with GPU and other adjustments\n",
        "lgbm_params = {'boosting_type': 'gbdt',             # Type of boosting (Gradient Boosted Decision Trees)\n",
        "               'objective': 'regression',           # Objective function for regression\n",
        "               'metric': 'rmse',                    # Evaluation metric: Root Mean Squared Error (RMSE)\n",
        "               'device': 'gpu',                     # Use GPU for training\n",
        "               'gpu_platform_id': 0,                # GPU platform ID (set as needed)\n",
        "               'gpu_device_id': 0,                  # GPU device ID (set as needed)\n",
        "               'num_leaves': 31,                    # Number of leaves in the tree\n",
        "               'learning_rate': 0.05,               # Learning rate\n",
        "               'n_estimators': 1000,                # Number of trees (estimators)\n",
        "               'max_depth': -1,                     # Maximum depth of trees (-1 means no limit)\n",
        "               'min_child_samples': 20,             # Minimum number of samples in a child node\n",
        "               'subsample': 0.8,                    # Ratio of samples for subsampling\n",
        "               'colsample_bytree': 0.8,             # Proportion of columns for subsampling\n",
        "               'reg_alpha': 0.1,                    # L1 Regularization\n",
        "               'reg_lambda': 0.1,                   # L2 Regularization\n",
        "               'verbose': -1                        # Verbosity level (-1 for silent)\n",
        "              }\n",
        "\n",
        "# Initializing and training the XGBoost model\n",
        "xgb_model = xgb.XGBRegressor(**xgb_params)\n",
        "\n",
        "#\n",
        "xgb_model.fit(X_train, y_train,\n",
        "              eval_set=[(X_test, y_test)],\n",
        "              early_stopping_rounds=10,\n",
        "              verbose=True)\n",
        "\n",
        "# Making predictions with the trained XGBoost model\n",
        "xgb_model_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Initializing and training the LightGBM model\n",
        "lgbm_model = lgb.LGBMRegressor(**lgbm_params)\n",
        "\n",
        "#\n",
        "lgbm_model.fit(X_train, y_train,\n",
        "               eval_set=[(X_test, y_test)],\n",
        "               callbacks=[log_evaluation(10)])\n",
        "\n",
        "# Making predictions with the trained LightGBM model\n",
        "lgbm_predictions = lgbm_model.predict(X_test)\n",
        "\n",
        "# Calculating RMSE for the LightGBM model\n",
        "lgbm_rmse = sqrt(mean_squared_error(y_test, lgbm_predictions))\n",
        "print(f\"LightGBM RMSE: {lgbm_rmse:.4f}\")\n",
        "\n",
        "# Calculating RMSE for the XGBoost model\n",
        "xgb_rmse = sqrt(mean_squared_error(y_test, xgb_model_pred))\n",
        "print(f\"XGBoost RMSE: {xgb_rmse:.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.490789Z",
          "iopub.status.idle": "2025-01-07T04:47:49.491175Z",
          "shell.execute_reply": "2025-01-07T04:47:49.491039Z"
        },
        "id": "_x95izwBQzsK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"1\"></a>\n",
        "# <div style=\"text-align:center; border-radius:15px 50px; padding:7px; color:white; margin:0; font-size:110%; font-family:Pacifico; background-color:#0073e6; overflow:hidden\"><b> Part 9 - Important Features</b></div>"
      ],
      "metadata": {
        "id": "XGLIhNs3QzsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Feature Importance for the XGBoost Model\n",
        "xgb.plot_importance(xgb_model, max_num_features=25, importance_type='weight')\n",
        "plt.title('Importance of Features - XGBoost')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.491833Z",
          "iopub.status.idle": "2025-01-07T04:47:49.492125Z",
          "shell.execute_reply": "2025-01-07T04:47:49.491996Z"
        },
        "id": "KO32IemLQzsK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the feature importances from the LightGBM model into a DataFrame\n",
        "lgbm_feature_importances = pd.DataFrame({'Feature': X_train.columns,\n",
        "                                         'Importance': lgbm_model.feature_importances_})\n",
        "\n",
        "# Sorting the features by their importance in descending order\n",
        "lgbm_feature_importances = lgbm_feature_importances.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plotting the top 20 most important features\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.barplot(x='Importance', y='Feature', data=lgbm_feature_importances.head(20))\n",
        "plt.title('Feature Importance - LightGBM')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.492945Z",
          "iopub.status.idle": "2025-01-07T04:47:49.493251Z",
          "shell.execute_reply": "2025-01-07T04:47:49.493109Z"
        },
        "id": "Dmu6pzbAQzsK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"1\"></a>\n",
        "# <div style=\"text-align:center; border-radius:15px 50px; padding:7px; color:white; margin:0; font-size:110%; font-family:Pacifico; background-color:#0073e6; overflow:hidden\"><b> Part 10 - Final result ML</b></div>"
      ],
      "metadata": {
        "id": "UGY8t13IQzsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from math import sqrt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Function to calculate MAPE\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "# Calculating the metrics for the LightGBM model\n",
        "lgbm_r2 = r2_score(y_test, lgbm_predictions)\n",
        "lgbm_rmse = sqrt(mean_squared_error(y_test, lgbm_predictions))\n",
        "lgbm_mape = mean_absolute_percentage_error(y_test, lgbm_predictions)\n",
        "\n",
        "# Calculating the metrics for the XGBoost model\n",
        "xgb_r2 = r2_score(y_test, xgb_model_pred)\n",
        "xgb_rmse = sqrt(mean_squared_error(y_test, xgb_model_pred))\n",
        "xgb_mape = mean_absolute_percentage_error(y_test, xgb_model_pred)\n",
        "\n",
        "# Storing the results in a dictionary\n",
        "results = {\"Model\": [\"LightGBM\", \"XGBoost\"],\n",
        "           \"R²\": [lgbm_r2, xgb_r2],\n",
        "           \"RMSE\": [lgbm_rmse, xgb_rmse],\n",
        "           \"MAPE (%)\": [lgbm_mape, xgb_mape]}\n",
        "\n",
        "# Converting the dictionary into a DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Displaying the DataFrame with the results\n",
        "results_df"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.4941Z",
          "iopub.status.idle": "2025-01-07T04:47:49.494498Z",
          "shell.execute_reply": "2025-01-07T04:47:49.494295Z"
        },
        "id": "p4TLsVxpQzsL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensembling (opcional)\n",
        "# Combine predictions from LightGBM and XGBoost using their average\n",
        "y_pred_ensemble = (lgbm_predictions + xgb_model_pred) / 2\n",
        "\n",
        "# Calculating MAPE for the ensemble predictions\n",
        "ensemble_mape = mean_absolute_percentage_error(y_test, y_pred_ensemble)\n",
        "\n",
        "# Displaying the MAPE of the ensemble model\n",
        "print(f\"MAPE (Ensemble) on validation: {ensemble_mape:.4f}%\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.495416Z",
          "iopub.status.idle": "2025-01-07T04:47:49.495774Z",
          "shell.execute_reply": "2025-01-07T04:47:49.495631Z"
        },
        "id": "VMwxetTfQzsL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "# Function to calculate MAPE\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "# Ensemble: Averaging predictions from LightGBM and XGBoost\n",
        "y_pred_ensemble = (lgbm_predictions + xgb_model_pred) / 2\n",
        "\n",
        "# Calculating metrics for the ensemble\n",
        "ensemble_rmse = sqrt(mean_squared_error(y_test, y_pred_ensemble))\n",
        "ensemble_mape = mean_absolute_percentage_error(y_test, y_pred_ensemble)\n",
        "\n",
        "# Printing the results\n",
        "print(f\"Ensemble RMSE: {ensemble_rmse:.4f}\")\n",
        "print(f\"Ensemble MAPE: {ensemble_mape:.4f}%\")\n",
        "\n",
        "# Adding ensemble results to the DataFrame\n",
        "results[\"Model\"].append(\"Ensemble\")\n",
        "results[\"R²\"].append(r2_score(y_test, y_pred_ensemble))\n",
        "results[\"RMSE\"].append(ensemble_rmse)\n",
        "results[\"MAPE (%)\"].append(ensemble_mape)\n",
        "\n",
        "# Converting to DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Displaying the results\n",
        "results_df"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.496426Z",
          "iopub.status.idle": "2025-01-07T04:47:49.49677Z",
          "shell.execute_reply": "2025-01-07T04:47:49.496583Z"
        },
        "id": "q-ZcIDeZQzsL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"1\"></a>\n",
        "# <div style=\"text-align:center; border-radius:15px 50px; padding:7px; color:white; margin:0; font-size:110%; font-family:Pacifico; background-color:#0073e6; overflow:hidden\"><b> Part 11 - Submission</b></div>"
      ],
      "metadata": {
        "id": "4FqCHS-EQzsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the test dataset\n",
        "test = pd.read_csv('/kaggle/input/playground-series-s5e1/test.csv')\n",
        "\n",
        "# Create derived columns in the test dataset\n",
        "test['year'] = pd.to_datetime(test['date']).dt.year\n",
        "test['month'] = pd.to_datetime(test['date']).dt.month\n",
        "test['day'] = pd.to_datetime(test['date']).dt.day\n",
        "test['dayofweek'] = pd.to_datetime(test['date']).dt.dayofweek  # Monday=0, Sunday=6\n",
        "test['is_weekend'] = test['dayofweek'].isin([5, 6]).astype(int)  # Weekend indicator\n",
        "test['weekofyear'] = pd.to_datetime(test['date']).dt.isocalendar().week  # Week of the year\n",
        "\n",
        "# Define initial feature columns\n",
        "feature_cols = [\n",
        "    'year', 'month', 'day', 'dayofweek', 'is_weekend', 'weekofyear',\n",
        "    'country_1', 'country_2', 'country_3', 'country_4', 'country_5', 'store_1',\n",
        "    'store_2', 'product_1', 'product_2', 'product_3', 'product_4'\n",
        "]\n",
        "\n",
        "# Adjust feature_cols to include only columns present in the test dataset\n",
        "feature_cols = [col for col in feature_cols if col in test.columns]\n",
        "\n",
        "# Create X_test (features for the test dataset)\n",
        "X_test = test[feature_cols]\n",
        "\n",
        "# Remove the 'date' column if it is not needed\n",
        "if 'date' in X_test.columns:\n",
        "    X_test = X_test.drop(columns=['date'])\n",
        "\n",
        "# Encode categorical columns using Label Encoding\n",
        "categorical_cols = ['country', 'store', 'product']\n",
        "for col in categorical_cols:\n",
        "    if col in X_test.columns:\n",
        "        le = LabelEncoder()\n",
        "        X_test[col] = le.fit_transform(X_test[col])\n",
        "\n",
        "# Ensure all columns are numeric\n",
        "X_test = X_test.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Ensure test has the same columns as the training set\n",
        "feature_cols_train = X_train.columns.tolist()\n",
        "\n",
        "# Add missing columns to test with a default value (e.g., 0)\n",
        "for col in feature_cols_train:\n",
        "    if col not in test.columns:\n",
        "        test[col] = 0\n",
        "\n",
        "# Select the feature columns from the test dataset\n",
        "X_test = test[feature_cols_train]\n",
        "\n",
        "# Reorder columns to match the training set\n",
        "X_test = X_test.reindex(columns=feature_cols_train, fill_value=0)\n",
        "\n",
        "# Remove the 'date' column again if it exists\n",
        "if 'date' in X_test.columns:\n",
        "    X_test = X_test.drop(columns=['date'])\n",
        "\n",
        "# Final check to ensure all columns are numeric\n",
        "X_test = X_test.apply(pd.to_numeric, errors='coerce')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.497533Z",
          "iopub.status.idle": "2025-01-07T04:47:49.497967Z",
          "shell.execute_reply": "2025-01-07T04:47:49.497774Z"
        },
        "id": "-agzi69YQzsM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure X_test has the same columns as X_train\n",
        "for col in X_train.columns:\n",
        "    if col not in X_test.columns:\n",
        "        X_test[col] = 0  # Add missing columns with default value\n",
        "for col in X_test.columns:\n",
        "    if col not in X_train.columns:\n",
        "        X_test = X_test.drop(columns=[col])  # Drop extra columns\n",
        "\n",
        "# Reorder columns to match the training set\n",
        "X_test = X_test[X_train.columns]\n",
        "\n",
        "# Make predictions\n",
        "y_pred_test_lgb = lgbm_model.predict(X_test)  # LightGBM\n",
        "y_pred_test_xgb = xgb_model.predict(X_test)  # XGBoost\n",
        "\n",
        "# Ensemble of predictions\n",
        "y_pred_test_ensemble = (y_pred_test_lgb + y_pred_test_xgb) / 2\n",
        "\n",
        "# Create the submission DataFrame with the correct column name 'Id'\n",
        "submission = pd.DataFrame({'id': test['id'],\n",
        "                           'Prediction': y_pred_test_ensemble})\n",
        "\n",
        "# Save the submission file\n",
        "submission.to_csv('submission_ensemble.csv', index=False)\n",
        "print(\"Submission successfully created! File saved as 'submission_ensemble.csv'.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-07T04:47:49.498617Z",
          "iopub.status.idle": "2025-01-07T04:47:49.498928Z",
          "shell.execute_reply": "2025-01-07T04:47:49.498807Z"
        },
        "id": "wpSo_f6mQzsM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "a55B8B_0QzsM"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}